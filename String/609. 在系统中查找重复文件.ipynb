{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "给定一个目录信息列表，包括目录路径，以及该目录中的所有包含内容的文件，您需要找到文件系统中的所有重复文件组的路径。一组重复的文件至少包括二个具有完全相同内容的文件。\n",
    "\n",
    "输入列表中的单个目录信息字符串的格式如下：\n",
    "\n",
    "\"root/d1/d2/.../dm f1.txt(f1_content) f2.txt(f2_content) ... fn.txt(fn_content)\"\n",
    "\n",
    "这意味着有 n 个文件（f1.txt, f2.txt ... fn.txt 的内容分别是 f1_content, f2_content ... fn_content）在目录 root/d1/d2/.../dm 下。注意：n>=1 且 m>=0。如果 m=0，则表示该目录是根目录。\n",
    "\n",
    "该输出是重复文件路径组的列表。对于每个组，它包含具有相同内容的文件的所有文件路径。文件路径是具有下列格式的字符串：\n",
    "\n",
    "\"directory_path/file_name.txt\"\n",
    "\n",
    "示例 1：\n",
    "\n",
    "输入：\n",
    "[\"root/a 1.txt(abcd) 2.txt(efgh)\", \"root/c 3.txt(abcd)\", \"root/c/d 4.txt(efgh)\", \"root 4.txt(efgh)\"]\n",
    "输出：  \n",
    "[[\"root/a/2.txt\",\"root/c/d/4.txt\",\"root/4.txt\"],[\"root/a/1.txt\",\"root/c/3.txt\"]]\n",
    " \n",
    "\n",
    "注：\n",
    "\n",
    "最终输出不需要顺序。\n",
    "您可以假设目录名、文件名和文件内容只有字母和数字，并且文件内容的长度在 [1，50] 的范围内。\n",
    "给定的文件数量在 [1，20000] 个范围内。\n",
    "您可以假设在同一目录中没有任何文件或目录共享相同的名称。\n",
    "您可以假设每个给定的目录信息代表一个唯一的目录。目录路径和文件信息用一个空格分隔。\n",
    " \n",
    "\n",
    "超越竞赛的后续行动：\n",
    "\n",
    "1.假设您有一个真正的文件系统，您将如何搜索文件？广度搜索还是宽度搜索？\n",
    "2.如果文件内容非常大（GB级别），您将如何修改您的解决方案？\n",
    "3.如果每次只能读取 1 kb 的文件，您将如何修改解决方案？\n",
    "4.修改后的解决方案的时间复杂度是多少？其中最耗时的部分和消耗内存的部分是什么？如何优化？\n",
    "5.如何确保您发现的重复文件不是误报？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Solution(object):\n",
    "    def findDuplicate(self, paths):\n",
    "        \"\"\"\n",
    "        :type paths: List[str]\n",
    "        :rtype: List[List[str]]\n",
    "        \"\"\"\n",
    "        # File = collections.namedtuple(\"File\",[\"path\",\"content\"])\n",
    "        # files = []\n",
    "        mapping = collections.defaultdict(list)\n",
    "        for path in paths:\n",
    "            tmp = path.split()\n",
    "            root = tmp[0]\n",
    "            for t in tmp[1:]:\n",
    "                file_name,content = t.split(\"(\")\n",
    "                content = content[:-1]\n",
    "                mapping[content].append(root+\"/\"+file_name)\n",
    "        return [item[1] for item in mapping.items() if len(item[1])>1]        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "超越竞赛的后续行动：\n",
    "\n",
    "1.假设您有一个真正的文件系统，您将如何搜索文件？广度搜索还是宽度搜索？\n",
    "2.如果文件内容非常大（GB级别），您将如何修改您的解决方案？\n",
    "3.如果每次只能读取 1 kb 的文件，您将如何修改解决方案？\n",
    "4.修改后的解决方案的时间复杂度是多少？其中最耗时的部分和消耗内存的部分是什么？如何优化？\n",
    "5.如何确保您发现的重复文件不是误报？\n",
    "\n",
    "1. bfs队列长度由目录下文件数量决定，即width，dfs堆栈长度由文件系统深度决定，即depth。\n",
    "    如果文件系统很大，采用dfs的话，堆栈会很深，内存可能溢出；采用bfs的话，队列可能会拉的很长，同样会占据很大的内存。\n",
    "      In a file system, BFS is like scan all the files under one directory, then go to the 1st child directory, 2nd child directory etc.\n",
    "      DFS instead, is visiting 1st child directory, 1st child directory's child (grandchild directory).. etc all the way down to the         bottom,then go back to 2nd child directory of the root. So it adds a lot of overhead of \"cd\" (change directory) operations.\n",
    "      会有额外的cd操作\n",
    "2. 文件内容很大，比较查重的步骤就会复杂度突增。可以先map文件大小，然后对文件大小相同的文件再进行hash（md5，SHA256）比较,hash值相同，再进行byte   by byte的比较\n",
    "3. 本质不变，那就1kb chunk地比较\n",
    "4. O(n^2*k)  k为文件平均大小。最耗时的部分是比较(size,hash,byte)，最耗内存的是hash。优化的话，找其他更优的hash算法\n",
    "5. File size, Hash and byte by byte comparisons."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
